{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import shutil\n",
    "from tensor_sampling_utils import sample_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:/Hamed/Projects/Python/License Plate Detection/VGG_coco_SSD_300x300_iter_400000_subsampled_37_classes.h5'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_source_path = 'E:/Hamed/Projects/Python/License Plate Detection/VGG_coco_SSD_300x300_iter_400000.h5'\n",
    "weights_destination_path = 'E:/Hamed/Projects/Python/License Plate Detection/VGG_coco_SSD_300x300_iter_400000_subsampled_37_classes.h5'\n",
    "shutil.copy(weightsSourcePath, weightsDestinationPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_source_file = h5py.File(weights_source_path, 'r')\n",
    "weights_destination_file = h5py.File(weights_destination_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_names = ['conv4_3_norm_mbox_conf',\n",
    "                    'fc7_mbox_conf',\n",
    "                    'conv6_2_mbox_conf',\n",
    "                    'conv7_2_mbox_conf',\n",
    "                    'conv8_2_mbox_conf',\n",
    "                    'conv9_2_mbox_conf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the 'conv4_3_norm_mbox_conf' weights:\n",
      "\n",
      "kernel:\t (3, 3, 512, 324)\n",
      "bias:\t (324,)\n"
     ]
    }
   ],
   "source": [
    "conv4_3_norm_mbox_conf_kernel = weights_source_file[classifier_names[0]][classifier_names[0]]['kernel:0']\n",
    "conv4_3_norm_mbox_conf_bias = weights_source_file[classifier_names[0]][classifier_names[0]]['bias:0']\n",
    "\n",
    "print(\"Shape of the '{}' weights:\".format(classifier_names[0]))\n",
    "print()\n",
    "print(\"kernel:\\t\", conv4_3_norm_mbox_conf_kernel.shape)\n",
    "print(\"bias:\\t\", conv4_3_norm_mbox_conf_bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set the number of classes in the source weights file. Note that this number must include\n",
    "#       the background class, so for MS COCO's 80 classes, this must be 80 + 1 = 81.\n",
    "n_classes_source = 81\n",
    "# TODO: Set the indices of the classes that you want to pick for the sub-sampled weight tensors.\n",
    "#       In case you would like to just randomly sample a certain number of classes, you can just set\n",
    "#       `classes_of_interest` to an integer instead of the list below. Either way, don't forget to\n",
    "#       include the background class. That is, if you set an integer, and you want `n` positive classes,\n",
    "#       then you must set `classes_of_interest = n + 1`.\n",
    "classes_of_interest = 37\n",
    "# classes_of_interest = 9 # Uncomment this in case you want to just randomly sub-sample the last axis instead of providing a list of indices.\n",
    "\n",
    "for name in classifier_names:\n",
    "    # Get the trained weights for this layer from the source HDF5 weights file.\n",
    "    kernel = weights_source_file[name][name]['kernel:0'].value\n",
    "    bias = weights_source_file[name][name]['bias:0'].value\n",
    "\n",
    "    # Get the shape of the kernel. We're interested in sub-sampling\n",
    "    # the last dimension, 'o'.\n",
    "    height, width, in_channels, out_channels = kernel.shape\n",
    "    \n",
    "    # Compute the indices of the elements we want to sub-sample.\n",
    "    # Keep in mind that each classification predictor layer predicts multiple\n",
    "    # bounding boxes for every spatial location, so we want to sub-sample\n",
    "    # the relevant classes for each of these boxes.\n",
    "    if isinstance(classes_of_interest, (list, tuple)):\n",
    "        subsampling_indices = []\n",
    "        for i in range(int(out_channels/n_classes_source)):\n",
    "            indices = np.array(classes_of_interest) + i * n_classes_source\n",
    "            subsampling_indices.append(indices)\n",
    "        subsampling_indices = list(np.concatenate(subsampling_indices))\n",
    "    elif isinstance(classes_of_interest, int):\n",
    "        subsampling_indices = int(classes_of_interest * (out_channels/n_classes_source))\n",
    "    else:\n",
    "        raise ValueError(\"`classes_of_interest` must be either an integer or a list/tuple.\")\n",
    "    \n",
    "    # Sub-sample the kernel and bias.\n",
    "    # The `sample_tensors()` function used below provides extensive\n",
    "    # documentation, so don't hesitate to read it if you want to know\n",
    "    # what exactly is going on here.\n",
    "    new_kernel, new_bias = sample_tensors(weights_list=[kernel, bias],\n",
    "                                          sampling_instructions=[height, width, in_channels, subsampling_indices],\n",
    "                                          axes=[[3]], # The one bias dimension corresponds to the last kernel dimension.\n",
    "                                          init=['gaussian', 'zeros'],\n",
    "                                          mean=0.0,\n",
    "                                          stddev=0.005)\n",
    "    \n",
    "    # Delete the old weights from the destination file.\n",
    "    del weights_destination_file[name][name]['kernel:0']\n",
    "    del weights_destination_file[name][name]['bias:0']\n",
    "    # Create new datasets for the sub-sampled weights.\n",
    "    weights_destination_file[name][name].create_dataset(name='kernel:0', data=new_kernel)\n",
    "    weights_destination_file[name][name].create_dataset(name='bias:0', data=new_bias)\n",
    "\n",
    "# Make sure all data is written to our output file before this sub-routine exits.\n",
    "weights_destination_file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the 'conv4_3_norm_mbox_conf' weights:\n",
      "\n",
      "kernel:\t (3, 3, 512, 148)\n",
      "bias:\t (148,)\n"
     ]
    }
   ],
   "source": [
    "conv4_3_norm_mbox_conf_kernel = weights_destination_file[classifier_names[0]][classifier_names[0]]['kernel:0']\n",
    "conv4_3_norm_mbox_conf_bias = weights_destination_file[classifier_names[0]][classifier_names[0]]['bias:0']\n",
    "\n",
    "print(\"Shape of the '{}' weights:\".format(classifier_names[0]))\n",
    "print()\n",
    "print(\"kernel:\\t\", conv4_3_norm_mbox_conf_kernel.shape)\n",
    "print(\"bias:\\t\", conv4_3_norm_mbox_conf_bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "\n",
    "from models.keras_ssd300 import ssd_300\n",
    "from keras_loss_function.keras_ssd_loss import SSDLoss\n",
    "from keras_layers.keras_layer_AnchorBoxes import AnchorBoxes\n",
    "from keras_layers.keras_layer_DecodeDetections import DecodeDetections\n",
    "from keras_layers.keras_layer_DecodeDetections2 import DecodeDetections2\n",
    "from keras_layers.keras_layer_L2Normalization import L2Normalization\n",
    "\n",
    "from data_generator.object_detection_2d_data_generator import DataGenerator\n",
    "from data_generator.object_detection_2d_photometric_ops import ConvertTo3Channels\n",
    "from data_generator.object_detection_2d_patch_sampling_ops import RandomMaxCropFixedAR\n",
    "from data_generator.object_detection_2d_geometric_ops import Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 300 # Height of the input images\n",
    "img_width = 300 # Width of the input images\n",
    "img_channels = 3 # Number of color channels of the input images\n",
    "subtract_mean = [123, 117, 104] # The per-channel mean of the images in the dataset\n",
    "swap_channels = [2, 1, 0] # The color channel order in the original SSD is BGR, so we should set this to `True`, but weirdly the results are better without swapping.\n",
    "# TODO: Set the number of classes.\n",
    "n_classes = 36 # Number of positive classes, e.g. 20 for Pascal VOC, 80 for MS COCO\n",
    "scales = [0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05] # The anchor box scaling factors used in the original SSD300 for the MS COCO datasets.\n",
    "# scales = [0.1, 0.2, 0.37, 0.54, 0.71, 0.88, 1.05] # The anchor box scaling factors used in the original SSD300 for the Pascal VOC datasets.\n",
    "aspect_ratios = [[1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5]] # The anchor box aspect ratios used in the original SSD300; the order matters\n",
    "two_boxes_for_ar1 = True\n",
    "steps = [8, 16, 32, 64, 100, 300] # The space between two adjacent anchor box center points for each predictor layer.\n",
    "offsets = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5] # The offsets of the first anchor box center points from the top and left borders of the image as a fraction of the step size for each predictor layer.\n",
    "clip_boxes = False # Whether or not you want to limit the anchor boxes to lie entirely within the image boundaries\n",
    "variances = [0.1, 0.1, 0.2, 0.2] # The variances by which the encoded target coordinates are scaled as in the original implementation\n",
    "normalize_coords = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model built.\n",
      "Weights file loaded: E:/Hamed/Projects/Python/License Plate Detection/VGG_coco_SSD_300x300_iter_400000_subsampled_37_classes.h5\n"
     ]
    }
   ],
   "source": [
    "# 1: Build the Keras model\n",
    "\n",
    "K.clear_session() # Clear previous models from memory.\n",
    "\n",
    "model = ssd_300(image_size=(img_height, img_width, img_channels),\n",
    "                n_classes=n_classes,\n",
    "                mode='inference',\n",
    "                l2_regularization=0.0005,\n",
    "                scales=scales,\n",
    "                aspect_ratios_per_layer=aspect_ratios,\n",
    "                two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                steps=steps,\n",
    "                offsets=offsets,\n",
    "                clip_boxes=clip_boxes,\n",
    "                variances=variances,\n",
    "                normalize_coords=normalize_coords,\n",
    "                subtract_mean=subtract_mean,\n",
    "                divide_by_stddev=None,\n",
    "                swap_channels=swap_channels,\n",
    "                confidence_thresh=0.5,\n",
    "                iou_threshold=0.45,\n",
    "                top_k=200,\n",
    "                nms_max_output_size=400,\n",
    "                return_predictor_sizes=False)\n",
    "\n",
    "print(\"Model built.\")\n",
    "\n",
    "# 2: Load the sub-sampled weights into the model.\n",
    "\n",
    "# Load the weights that we've just created via sub-sampling.\n",
    "weights_path = weights_destination_path\n",
    "\n",
    "model.load_weights(weights_path, by_name=True)\n",
    "\n",
    "print(\"Weights file loaded:\", weights_path)\n",
    "\n",
    "# 3: Instantiate an Adam optimizer and the SSD loss function and compile the model.\n",
    "\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "\n",
    "model.compile(optimizer=adam, loss=ssd_loss.compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
